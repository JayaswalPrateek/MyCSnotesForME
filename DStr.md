 ***Big O***
- O(1) are the most efficient algorithms as they scale really well because the execution time is independent of the input size
	- like searching a value in map using its key.
- O(n) means the execution time is linearly dependent on the input size
	- searching for an element in an array
	- we usually take the worst case scenario like if the element we are looking for is at the end of the array for an array of length n then its O(n)
	- O(n) can be O(1) if the element we are looking for is at the 0th index but this is not guaranteed
- nested loop doesn't always imply O(n^2) 
- but O(n^2) can have nested loops
- if i am searching i 1-D loop twice it is O(2n)
	- so if i doing it n times it is O(n^2)
- suppose we have a 1-D array of length n and first we traverse from 0th index to (n-1)th index
	- then in the 2nd pass we skip the 0th element and start from 1st element. next pass from 2nd one and so on.
	- so we will have a total of n passes for an array of length n
	- if we draw the array one below the other for every pass and erase the block we skip, we will get 
		```
		------------,
		\			|
		  \			|
			\		|
			  \		|
			    \   |
				  \ |
		```
		- this has base and height n. the square had a O(n^2) this has O(n^2/2) but constant doesn't matter so its still O(n^2)
- if a 2-D matrix is not a square and has sides nxm then it will have O(n x M) 
- O(log n)
	- we keep shrinking the array in half so we will never have to traverse all the n elements and always less than n so it will be O(log n)
	- so n=2^x where n is the length and x is the no of times will have to split in half so take log both sides to get O(log n)
	- also used for binary trees
	- very efficient
- O(n log n ) is more efficient than 0(n^2) but less than O(n)
- imagine a tree which has 3 branches and each branch has 3 more branches and so on and if the height of the tree is n then O(3^n)
- O(n!) is very very bad and inefficient
- Efficiency increases from left to right


![time vs input size](https://raw.githubusercontent.com/JayaswalPrateek/MyCSnotesForME/main/Attachments/Screenshot%20from%202023-01-21%2009-52-29.png)

***Bird Eye View***
- Arrays
	- Read is O(1) but inserting and deleting is O(n)
	- Reason: contiguous memory allocation is done so if next address is occupied then the array needs to be copied to another location contiguously
	can have unused memory
- singly Linked list:
	- opposite of array
	- read is O(n), insert and delete is O(1) if at the beginning, O(n) if at the end
	- they dont have indexes like arrays
	- address of the head node is stored and used and the last nodes points to null
	- no unused memory but pointer needs memory
- doubly linked list:
	- ![doubly linked list](https://raw.githubusercontent.com/JayaswalPrateek/MyCSnotesForME/main/Attachments/DLL1.png)
- hash maps are like arrays but indices can be of any data type
	- read insert delete are all O(1)
- stack: stack of washed plates
	- lifo
	- push,pop and peek are O(1)
	- push adds new element at the top of the stack
	- pop removes the topmost element
	- peek reads the topmost element
	- useful for reversing without recursion
	- inset and delete happen from the same end
- queue 
	- fifo
	- enqueue adds new element at the end of the queue O(1)
	- dequeue is the pop for the stack removes front most element O(1)
	- front reads the frontmost element O(1)
	- more used than stack
	- insert happens at one end and delete happens from other
	- used when there is a shared resource and it can only handle only one resource at a time like printer or processor
- binary search trees
	- a binary search tree is a binary tree that has the left child node lesser than or equal to the parent node and the right child node larger than or equal to the parent node
	- a right child node of a left parent node cannot be greater than the grandparent node 
	- used for searching efficiently
	- recursive data structure
		- as tree has unidirectional links, recursion allows to maintain previous nodes in stack's activation record
	- a tree with n nodes has n-1 links always. used to check validity of tree
	- depth and height
		- root has depth 0, depth measured from topmost root
		- height of some node is the number of links to reach it from the furthest child so measured from leaf(childless node) and height of a leaf=0 
			- ht of tree=ht of node	
	 - a tree where every node can have only 2 child nodes and the left child node should be lesser than parent node and the right child node should be greater than the parent node 
	 - number of nodes at height h = 2^h so we can find max number of nodes of the BINARY tree if we know the ht of the BINARY tree
	 - for a BINARY tree of ht h, max nodes = 2^h - 1
	 - a binary tree where a node can only have 2 or no nodes is called a perfect binary tree and its height is floor of log base2 n 
	 - cost proportional to ht of tree
		 - ht is less if the tree is dense and more like a perfect binary tree
		 - opposite of a perfect binary tree is a linked list
			 - min ht can be floor of log base2 n
			 - max ht is n-1
			 - n is the number of nodes
		- so tree has O(ht)
			- perfect hai toh O(log base2 n)
			- linked list toh O(n)
		- ht of empty tree = -1 as no node
		- ht of tree w/ 1 node is 0
		- for n nodes its n-1 which is same as number of links
	- try keeping tree balanced, balanced tree is one for which (ht of left child subtree of parent) - (ht of right child subtree of parent) = 0 or 1
	- ![binary tree array](https://raw.githubusercontent.com/JayaswalPrateek/MyCSnotesForME/main/Attachments/Screenshot%20from%202023-01-23%2022-31-43.png)
	- ![why use binary search tree](https://raw.githubusercontent.com/JayaswalPrateek/MyCSnotesForME/main/Attachments/Screenshot%20from%202023-01-23%2023-18-36.png)
	- in a binary search tree at every step you compare the value you are looking for with the value at the node and if it is match you stop
		- else you discard left or right sub tree
	- ![this is insertion](https://raw.githubusercontent.com/JayaswalPrateek/MyCSnotesForME/main/Attachments/Screenshot%20from%202023-01-23%2023-29-47.png)
- graphs 
	- linked lists and binary search trees are also types of graphs
	- no restrictions than linked lists and binary search trees
	- G=(V,E) where G is an ordered pair of vertices and edges
		- every edge can be directed(one way) or undirected(2 way)
	- an edge can have weight like distance between 2 places
		- with weights we cannot just rely on the number of edges between 2 vertices to find the shortest path
		- when no weights given assume all weights are equal to 1
	- a self loop means the vertex has an edge that points to itself
		- many websites have links to the same page and that is an example of self loop
	- multi edge when there are multiple edges between 2 vertices like airlines between 2 airports 
	- a simple graph has no self loop or multiedges
	- in a simple directed graph the max number of edges for n vertices is n(n-1) where n is not -ve
		- for simple undirected it will be n(n-1)/2
	- a dense graph has edges near to max possible else it is sparse
	- for dense graph we use adjacency matrix and for sparse it it an adjacency list 
	- path
		- simple path when vertices/edges are not repeated
		- if repeated it is a walk
			- closed walk when start and end is the same vertex also called cycle
				- tree is acyclic
		- trail when edges are not repeated but vertices can be
		- creating graph data str in the memory
			- use 2 vectors
			- one stores the content of the vertex called vertex list
			- and edge list which is a struct of edge (and it weight also if needed) and stores 2 ints
			- we know every vertex stored in the vertex list vector has an index
			- we can imagine that an edge connects these indices instead of pointing to them
			- ![creating a graph](https://raw.githubusercontent.com/JayaswalPrateek/MyCSnotesForME/main/Attachments/Screenshot%20from%202023-01-25%2014-25-06.png)
		- common operations
			- finding adjacent nodes
			- checking if 2 nodes are connected
			- every time this will take O(number of Edges or |E|)
				- this is nearly O(n^2) as we know the formula for max number of edges from the number of nodes
			- a O(n) is considered acceptable: which technically is O(number of vertices or |V|)
			- Solution: Adjacency Matrix and Adjacency List
			- Adjacency Matrix of order |V| x |V|
				- better for dense graphs beacuse we will store many 0s for a sparse graph
				- huge storage footprint
				- rows are corresponding to the vertex list and 1 in columns represent that they are connected
				- remaining values are set to zero
				- for undirected grpah the matrix will be symmetric so Aij == Aji so we can omit upper or lower triangular matrix with diagonal of matrix as the hypotenuse
				- ![adjacency matrix of a graph](https://raw.githubusercontent.com/JayaswalPrateek/MyCSnotesForME/main/Attachments/Screenshot%20from%202023-01-25%2014-39-31.png)
				- to find adjacent nodes we do a linear search on the vertex list and then do a linear search on the row of adjacency matrix corresponding to the index in vertex list
				- so it is O(V+V) = O(2V) = O(V)
				- to find if 2 nodes are connected or not we can pass their indices i and j we just need to find the value of `A[i][j]` in the Adjacency matrix and see if it is 0 or 1
				- so its a O(1)
					- if the names are given then we need to do a linear search on the vertex list making it O(V)
						- this can be avoided by using hash map so it will always be O(1)
				- for weighted graphs:
					- replace 1 with edge weights and 0 with INT_MAX
				- for facebook, using adjacent matrix optimally means for 1 billion users everyone is a friend of everyone else
					- this is impossible implying we will waste lot of memory
				- we are storing what nodes are connected to each other and also the ones that are not
					- this is redundant info as it can be deduced from knowing what nodes are connected to each other
			- adjacency list is an array of pointers
				- less memory as space complexity is O(number of edges) as e << v^2
				- ![Adjacency List](https://raw.githubusercontent.com/JayaswalPrateek/MyCSnotesForME/main/Attachments/Screenshot%20from%202023-01-25%2021-00-55.png)
				- we need to perform linear search or binary search on the adjacency list so for finding if 2 nodes are connected O(v) or O(log v)
					- v is the number of vertices
					- O(1) in the matrix
				- for finding adjacent nodes (all the neighbours of a node) then O(v)
					- also O(v) in matrix
				- 
